{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-*- coding: utf-8 -*-\n",
    "\n",
    "# Note Detection Console\n",
    "\n",
    "Console template created on May 23 2014, class taken from the SciPy 2015 Vispy talk opening example <br>\n",
    "see https://github.com/vispy/vispy/pull/928 @author: florian <br>\n",
    "\n",
    "librosa: https://librosa.github.io/librosa/generated/librosa.feature.chroma_cqt.html#librosa.feature.chroma_cqt <br>\n",
    "\n",
    "\n",
    "## HYPERPARAMETERS in the program\n",
    "(not necessarily easily changed)\n",
    "* chunksize\n",
    "elaborated in class MicrophoneRecorder\n",
    "* noise (for energy)\n",
    "determines the onset detection desensitivity\n",
    "* threshold crossing point\n",
    "determines the onset detection sensitivity\n",
    "* lower threshold of spectrum\n",
    "muting everything below the frequency\n",
    "\n",
    "## ISSUES:\n",
    "* lost frames, especially after updating graph\n",
    "I have no idea how to make the plotting happen on a separate thread\n",
    "\n",
    "* harmonic problem:\n",
    "lower frequencies: identifies the second harmonic as ffreq <br>\n",
    "higher frequencies: identifies half of ffreq as ffreq <br>\n",
    "no note may be identified if the drum and piano is hit at the same time <br>\n",
    "\n",
    "## TO DO LIST:\n",
    "* easier on-off switches\n",
    "* display every frame\n",
    "* exclude inharmonic sounds\n",
    "* subtracting the spectrum before the onset\n",
    "* multiplying the signal to analyse with a window\n",
    "* changing some terminologies: \"shift\" to \"delay\"\n",
    "* add some weights, adapt spread adapt spread based on how high ffreq is\n",
    "* implement new pitch (identify region) precise pitch (specify point) HPS to improve the alogrithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import threading\n",
    "import atexit\n",
    "import pyaudio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from PyQt4 import QtGui, uic, QtCore\n",
    "from matplotlib.backends.backend_qt4agg import FigureCanvasQTAgg as FigureCanvas\n",
    "from matplotlib.backends.backend_qt4agg import NavigationToolbar2QT as NavigationToolbar\n",
    "\n",
    "\n",
    "class MicrophoneRecorder(object):\n",
    "    \"\"\"\n",
    "    This microphone runs on an independent thread.\n",
    "    It groups the signal in blocks of 2048 (chunksize) entries, as \"frame\"\n",
    "    It accumulate these frames until \"get_frames\" is called, when it will pass over these entries.\n",
    "    (There should be no accumulation of entries, else information is lost.)\n",
    "    Choice of chunksize\n",
    "    - large enough to determine the exact frequency\n",
    "    - small enough to be responsive: to indicate the new note as promptly as possible\n",
    "    \"\"\"\n",
    "    def __init__(self, rate=44100, chunksize=2048):\n",
    "        the_input_device_index = 1  # to choose the microphone\n",
    "        self.rate = rate  # sampling rate of microphone\n",
    "        self.chunksize = chunksize  # size of each \"frames\"\n",
    "        self.p = pyaudio.PyAudio()  # imported object to interface with the microphone\n",
    "        self.stream = self.p.open(format=pyaudio.paInt16,  # sound take the format of int16\n",
    "                                  channels=1,  # takes mono?\n",
    "                                  rate=self.rate,  # sampling rate\n",
    "                                  input=True,\n",
    "                                  input_device_index=the_input_device_index,  # to choose the microphone\n",
    "                                  frames_per_buffer=self.chunksize,  # size of each \"frame\"\n",
    "                                  stream_callback=self.new_frame)  # function to call per \"frame\" generated\n",
    "        print self.p.get_device_info_by_index(the_input_device_index)[\"name\"]  # print mic name\n",
    "\n",
    "        self.lock = threading.Lock()  # something to do with threading\n",
    "        self.stop = False\n",
    "        self.frames = []  # initiatlize frames\n",
    "        atexit.register(self.close)\n",
    "\n",
    "    def new_frame(self, data, frame_count, time_info, status):\n",
    "        \"\"\"\n",
    "        function to call per \"frame\" generated\n",
    "        each frame has \"data\"\n",
    "        \"\"\"\n",
    "        data = np.fromstring(data, 'int16')\n",
    "        with self.lock:  # using threading?\n",
    "            self.frames.append(data)  # add data to the array of \"frames\"\n",
    "            if self.stop:\n",
    "                return None, pyaudio.paComplete\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def get_frames(self):\n",
    "        with self.lock:  # using threading?\n",
    "            frames = self.frames  # return the frames accumulated - should have only one\n",
    "            self.frames = []  # clear frames\n",
    "            return frames\n",
    "\n",
    "    def start(self):\n",
    "        self.stream.start_stream()  # opening recording stream\n",
    "\n",
    "    def close(self):  # some closing procedure, perhaps to erase memory\n",
    "        with self.lock:\n",
    "            self.stop = True\n",
    "        self.stream.close()\n",
    "        self.p.terminate()\n",
    "\n",
    "\n",
    "class MplFigure(object):  # don't know what is this for\n",
    "    def __init__(self, parent):\n",
    "        self.figure = plt.figure(facecolor='white')\n",
    "        self.canvas = FigureCanvas(self.figure)\n",
    "        self.toolbar = NavigationToolbar(self.canvas, parent)\n",
    "\n",
    "\n",
    "class LiveFFTWidget(QtGui.QWidget):\n",
    "    def __init__(self):\n",
    "        QtGui.QWidget.__init__(self)\n",
    "\n",
    "        self.chunksize = 2048\n",
    "        self.tempo_res = 32  # r_coeff resolution, needs to be a factor of chunksize\n",
    "        self.tempo_num = int(self.chunksize/self.tempo_res)  # interval between r_coeff_calculation\n",
    "        self.iteration = 0  # for counting, if needed\n",
    "        self.noise = np.round(200000*np.random.randn(self.chunksize))  # to desensitise onset detection\n",
    "        self.sampling_rate = 44100\n",
    "        self.notes_dict = [\"C\", \"C#\", \"D\", \"D#\", \"E\", \"F\", \"F#\", \"G\", \"G#\", \"A\", \"A#\", \"B\"]\n",
    "\n",
    "        # XKCD\n",
    "        self.score = [\"C\", \"E\", \"F\", \"G\", \"A\", \"G\", \"G\", \"C\"]\n",
    "        self.rythmn = [0., 1., 0.5, 1., 0.5, 1., 1., 1.]\n",
    "        self.score_numerical = [self.notes_dict.index(self.score[n]) for n in range(len(self.score))]\n",
    "        self.score_numerical = [0, 4, 5, 7, 9, 7, 7, 12]\n",
    "        self.notes_played = np.zeros([2, len(self.score_numerical)])\n",
    "        self.note_played_num = 0\n",
    "\n",
    "        self.indicate_frames_skipped = False\n",
    "        self.indicate_inharmonic = False\n",
    "        self.full_note_information = False\n",
    "\n",
    "        self.game_mode = True\n",
    "        if not self.game_mode:\n",
    "            self.indicate_frames_skipped = True\n",
    "            self.indicate_inharmonic = True\n",
    "            self.full_note_information = True\n",
    "\n",
    "        # holding variables\n",
    "        self.signal_frame_pp0 = self.noise\n",
    "        self.signal_frame_pp1 = self.noise\n",
    "        self.signal_frame_pp2 = self.noise\n",
    "        self.signal_frame_pp3 = self.noise\n",
    "\n",
    "        self.energy_frame_pp0 = self.noise\n",
    "        self.energy_frame_pp1 = self.noise\n",
    "        self.energy_frame_pp2 = self.noise\n",
    "        self.energy_frame_pp3 = self.noise\n",
    "\n",
    "        self.rcoeff_frame_pp1 = [0.0] * int(self.tempo_res)\n",
    "        self.rcoeff_frame_pp2 = [0.0] * int(self.tempo_res)\n",
    "        self.rcoeff_frame_pp3 = [0.0] * int(self.tempo_res)\n",
    "\n",
    "        self.note_detected = False\n",
    "        self.ffreq = 0.0\n",
    "        self.signal_to_show = [0] * (self.chunksize*2)\n",
    "        self.signal_to_ayse = [0] * self.chunksize\n",
    "        self.shift = 0.0  # affects the part of the signal that will be analysed and plotted\n",
    "\n",
    "        # customize the UI\n",
    "        self.initUI()\n",
    "\n",
    "        # init class data\n",
    "        self.initData()\n",
    "\n",
    "        # connect slots\n",
    "        self.connectSlots()  # don't know what is this for\n",
    "\n",
    "        # init MPL widget\n",
    "        self.initMplWidget()  # (refer to MplFigure class)\n",
    "\n",
    "        self.start_time = time.time()  # start timer\n",
    "        self.prev_time = time.time()  # to calculate the time difference\n",
    "\n",
    "    def initUI(self):  # comment on this later\n",
    "        vbox = QtGui.QVBoxLayout()\n",
    "\n",
    "        # mpl figure\n",
    "        self.main_figure = MplFigure(self)\n",
    "        vbox.addWidget(self.main_figure.toolbar)\n",
    "        vbox.addWidget(self.main_figure.canvas)\n",
    "\n",
    "        self.setLayout(vbox)\n",
    "\n",
    "        self.setGeometry(300, 300, 350, 300)\n",
    "        self.setWindowTitle('LiveFFT')\n",
    "        self.show()\n",
    "\n",
    "        timer = QtCore.QTimer()\n",
    "        timer.timeout.connect(self.handleNewData)  # calls handleNewData every 20ms\n",
    "        timer.start(20)  # chunks come out at a frequency of approximately 46ms\n",
    "        # keep reference to timer\n",
    "        self.timer = timer\n",
    "\n",
    "    def initData(self):\n",
    "        mic = MicrophoneRecorder(rate=44100, chunksize=self.chunksize)\n",
    "        mic.start()\n",
    "\n",
    "        # keeps reference to mic\n",
    "        self.mic = mic\n",
    "\n",
    "        # computes the parameters that will be used during plotting\n",
    "        self.freq_vect = np.fft.rfftfreq(mic.chunksize, 1./mic.rate)  # original\n",
    "        self.time_vect = np.arange(-mic.chunksize, mic.chunksize, dtype=np.float32) / mic.rate * 1000\n",
    "        # the onset will be in the middle\n",
    "\n",
    "    def connectSlots(self):\n",
    "        pass  # don't know what is this for\n",
    "\n",
    "    def initMplWidget(self):\n",
    "        \"\"\"\n",
    "        creates initial matplotlib plots in the main window and keeps references for further use\n",
    "        \"\"\"\n",
    "        # top plot: currently to show energy\n",
    "        self.ax_top = self.main_figure.figure.add_subplot(211)\n",
    "        self.ax_top.set_ylim(-32768, 32768)  # original\n",
    "        self.ax_top.set_xlim(self.time_vect.min(), self.time_vect.max())\n",
    "        self.ax_top.set_xlabel(u'time (ms)', fontsize=6)\n",
    "\n",
    "        # bottom plot: currently to show spectrum\n",
    "        self.ax_bottom = self.main_figure.figure.add_subplot(212)\n",
    "        self.ax_bottom.set_ylim(0, 1)\n",
    "        self.ax_bottom.set_xlim(0, 5000.)\n",
    "        self.ax_bottom.set_xlabel(u'frequency (Hz)', fontsize=6)\n",
    "\n",
    "        # line objects\n",
    "        self.line_top, = self.ax_top.plot(self.time_vect,\n",
    "                                          np.ones_like(self.time_vect), lw=0.2)\n",
    "\n",
    "        self.line_bottom, = self.ax_bottom.plot(self.freq_vect,\n",
    "                                                np.ones_like(self.freq_vect), lw=0.5)\n",
    "\n",
    "        self.pitch_line, = self.ax_bottom.plot((self.freq_vect[self.freq_vect.size / 2],\n",
    "                                                self.freq_vect[self.freq_vect.size / 2]),\n",
    "                                               self.ax_bottom.get_ylim(), lw=2)\n",
    "        # This plots for vertical line that marks the pitch\n",
    "        # plt.tight_layout()  # tight layout\n",
    "\n",
    "    def handleNewData(self):\n",
    "        \"\"\" handles the asynchronously collected sound chunks \"\"\"\n",
    "        signal_frames = self.mic.get_frames()\n",
    "\n",
    "        if len(signal_frames) > 0:\n",
    "            if len(signal_frames) and self.indicate_frames_skipped > 1:\n",
    "                print str(len(signal_frames) - 1) + \" frame lost\"\n",
    "                # indicate number of frames lost - should not have any\n",
    "            self.signal_frame_pp0 = signal_frames[-1]  # keeps only the last frame\n",
    "\n",
    "            # to calculate the rectangular window for every sample\n",
    "            # numpy operations are more efficient than using python loops\n",
    "            # the size of the rectangular window is one chunksize\n",
    "            # convolution can be considered\n",
    "            self.energy_frame_pp0 = np.full(self.chunksize, sum(np.absolute(self.signal_frame_pp2)), dtype=\"int32\")\n",
    "            to_cumsum = np.add(np.absolute(self.signal_frame_pp1), -np.absolute(self.signal_frame_pp2))\n",
    "            cumsum = np.cumsum(to_cumsum)\n",
    "            self.energy_frame_pp0[1:] = np.add(self.energy_frame_pp0[1:], cumsum[:-1])\n",
    "            self.energy_frame_pp0 = np.add(self.energy_frame_pp0, self.noise)\n",
    "\n",
    "            # calculating pearson correlation coefficient at 2048/32 samples\n",
    "            # to determine exact time of onset\n",
    "            # could not think of any way this could be parallelised\n",
    "            energy_arg = np.concatenate((self.energy_frame_pp1, self.energy_frame_pp0))\n",
    "            for i in range(self.tempo_res):\n",
    "                self.rcoeff_frame_pp1[i] = np.corrcoef(energy_arg[i*self.tempo_num:(i*self.tempo_num+self.chunksize)],\n",
    "                                                       np.arange(self.chunksize))[0, 1]\n",
    "\n",
    "            # print str(time.time() - self.start_time) + \"  \" + str(time.time() - self.prev_time) + \\\n",
    "            # \" detecting new note\"\n",
    "            # self.prev_time = time.time()\n",
    "            rcoeff_arg = np.concatenate((self.rcoeff_frame_pp2, self.rcoeff_frame_pp1))\n",
    "            # we need the previous rcoeff frame to determine onset\n",
    "\n",
    "            # finding the onset, any way not to loop?\n",
    "            for i in range(self.tempo_res, 0, -1):\n",
    "                # if rcoeff_arg[-i] > 0.80 and all(i < 0.80 for i in rcoeff_arg[-i-5:-i]):\n",
    "                if rcoeff_arg[-i] > 0.80 and np.max(rcoeff_arg[-i-31:-i]) < 0.80:\n",
    "                    # to determine onset  - where the rcoeff graph crosses 0.80,\n",
    "                    # 31 entries cooldown - check that previous entries do not have cooldown\n",
    "                    time_arg = np.concatenate((self.signal_frame_pp3, self.signal_frame_pp2,\n",
    "                                               self.signal_frame_pp1, self.signal_frame_pp0))\n",
    "                    self.signal_to_show = time_arg[-i*self.tempo_num - int((2+self.shift)*self.chunksize):\n",
    "                                                   -i*self.tempo_num - int((0+self.shift)*self.chunksize)]\n",
    "                    self.signal_to_ayse = time_arg[-i*self.tempo_num - int((1+self.shift)*self.chunksize):\n",
    "                                                   -i*self.tempo_num - int((0+self.shift)*self.chunksize)]\n",
    "                    signal_to_deduct = time_arg[-i*self.tempo_num - int((2+self.shift)*self.chunksize):\n",
    "                                                -i*self.tempo_num - int((1+self.shift)*self.chunksize)]\n",
    "                    # Consider whether should a window be applied\n",
    "\n",
    "                    spectrum = np.absolute(np.fft.fft(self.signal_to_ayse))\n",
    "                    spectrum_to_deduct = np.absolute(np.fft.fft(signal_to_deduct))\n",
    "                    to_subtract = False  # take the spectral difference between the current and previous chunk\n",
    "                    if to_subtract:\n",
    "                        spectrum = np.clip(np.add(spectrum, -1 * np.array(spectrum_to_deduct)), 0, 100000000)\n",
    "                        # consider the effectiveness of taking the difference\n",
    "\n",
    "                    # following is the hps algorithm\n",
    "                    spectrum[:12] = 0.0  # anything below middle C is muted\n",
    "                    spectrum[1024:] = 0.0  # mute second half of spectrum, lazy to change code\n",
    "\n",
    "                    scale1 = [0.0] * (2048 * 6)\n",
    "                    scale2 = [0.0] * (2048 * 6)\n",
    "                    scale3 = [0.0] * (2048 * 6)\n",
    "\n",
    "                    # upsampling the original scale spectrum, 6 for 1\n",
    "                    scale1_f1 = np.convolve(spectrum, [5.0 / 6.0, 1.0 / 6.0])[1:]\n",
    "                    scale1_f2 = np.convolve(spectrum, [4.0 / 6.0, 2.0 / 6.0])[1:]\n",
    "                    scale1_f3 = np.convolve(spectrum, [3.0 / 6.0, 3.0 / 6.0])[1:]\n",
    "                    scale1_f4 = np.convolve(spectrum, [2.0 / 6.0, 4.0 / 6.0])[1:]\n",
    "                    scale1_f5 = np.convolve(spectrum, [1.0 / 6.0, 5.0 / 6.0])[1:]\n",
    "                    scale1[::6] = spectrum\n",
    "                    scale1[1::6] = scale1_f5\n",
    "                    scale1[2::6] = scale1_f4\n",
    "                    scale1[3::6] = scale1_f3\n",
    "                    scale1[4::6] = scale1_f2\n",
    "                    scale1[5::6] = scale1_f1\n",
    "                    # downsampling from the 6 for 1 upsample\n",
    "                    scale2[:2048 * 3] = scale1[::2]\n",
    "                    scale3[:2048 * 2] = scale1[::3]\n",
    "                    hps = np.prod((scale1, scale2, scale3), axis=0)  # the \"product\" in harmonic product spectrum\n",
    "                    hps_max = np.argmax(hps)  # determine the location of the peak of hps result\n",
    "\n",
    "                    # calculate the corresponding frequency of the peak\n",
    "                    self.ffreq = hps_max * 44100.0 / (2048.0 * 6.0)  # sampling rate / (chunksize * upsampling value)\n",
    "\n",
    "                    self.spectrum = np.array(spectrum[:int(0.5*self.chunksize)+1])\n",
    "\n",
    "                    # TODO: add some weights, adapt spread based on how high ffreq is\n",
    "                    total_energy = np.sum(scale1)\n",
    "                    total_energy_due_to_ffreq = np.sum(scale1[::hps_max]) \\\n",
    "                                                + np.sum(scale1[1::hps_max]) + np.sum(scale1[:hps_max - 1:hps_max]) \\\n",
    "                                                # + np.sum(scale1[2::hps_max]) + np.sum(scale1[:hps_max - 2:hps_max]) \\\n",
    "                                                # + np.sum(scale1[3::hps_max]) + np.sum(scale1[:hps_max - 3:hps_max]) \\\n",
    "                                                # + np.sum(scale1[4::hps_max]) + np.sum(scale1[:hps_max - 4:hps_max]) \\\n",
    "                                                # + np.sum(scale1[5::hps_max]) + np.sum(scale1[:hps_max - 5:hps_max]) \\\n",
    "                                                # + np.sum(scale1[6::hps_max]) + np.sum(scale1[:hps_max - 6:hps_max])\n",
    "\n",
    "                    portion_of_energy = (total_energy_due_to_ffreq/total_energy)*20\n",
    "\n",
    "                    if portion_of_energy > 1:\n",
    "                        # printing note in solfage form\n",
    "                        note_no = -3 + (np.log2(self.ffreq) - np.log2(220.0)) * 12.0  # take logarithm and find note\n",
    "                        note_no_rounded = np.round(note_no)  # round off to nearest note\n",
    "                        note_no_difference = note_no - note_no_rounded\n",
    "                        octave_no = 4 + int(note_no_rounded // 12)\n",
    "                        solfate_no = int(note_no_rounded) % 12\n",
    "                        self.note = str(self.notes_dict[solfate_no]) + str(octave_no)\n",
    "                        time_played = time.time() - self.start_time\n",
    "\n",
    "                        if self.full_note_information:\n",
    "                            print (\"{:.2f}Hz({:02}) {:.2f}, {:3s} {:+.2f} at {:.3f}s\"\n",
    "                                   .format(self.ffreq, int(note_no_rounded), portion_of_energy,\n",
    "                                           self.note, note_no_difference, time_played))\n",
    "                        else:\n",
    "                            sys.stdout.write(self.notes_dict[solfate_no])\n",
    "                            sys.stdout.write(' ')\n",
    "\n",
    "                        self.note_detected = True\n",
    "\n",
    "                        if self.game_mode:\n",
    "                            if self.note_played_num < len(self.score_numerical):\n",
    "                                self.notes_played[0][self.note_played_num] = time_played\n",
    "                                self.notes_played[1][self.note_played_num] = note_no_rounded\n",
    "                                self.note_played_num += 1\n",
    "                                # print(self.notes_played)\n",
    "\n",
    "                            if self.note_played_num == len(self.score_numerical):\n",
    "                                self.note_played_num += 1\n",
    "                                score_difference = np.add(self.notes_played[1][:], -1 * np.array(self.score_numerical))\n",
    "                                score_diff = np.sum(np.absolute(score_difference))\n",
    "                                if score_diff == 0.0:\n",
    "                                    print(\"  perfect pitch!\")\n",
    "                                else:\n",
    "                                    print(\"  pitch errors totalling {} semitones\".format(score_diff))\n",
    "\n",
    "                                bar_time = 0.25 * (self.notes_played[0][5] - self.notes_played[0][0])  # edison specific\n",
    "                                rythmn_ideal = np.add(bar_time * np.array(np.cumsum(self.rythmn)),\n",
    "                                                      [self.notes_played[0][0]] * len(self.score_numerical))\n",
    "                                rythmn_difference = np.add(self.notes_played[0][:], -1 * np.array(rythmn_ideal))\n",
    "                                rythmn_diff = np.sum(np.absolute(rythmn_difference))\n",
    "                                print(\"rythmnic difference totalling {:.4f} seconds\".format(rythmn_diff))\n",
    "\n",
    "                                plt.clf()\n",
    "\n",
    "                                plt.plot()\n",
    "                                plt.scatter(rythmn_ideal, self.score_numerical,\n",
    "                                            s=[60] * len(self.notes_played), marker='o', color='b')\n",
    "                                plt.scatter(self.notes_played[0], self.notes_played[1],\n",
    "                                            s=[300] * len(self.notes_played), marker='x', color='r')\n",
    "\n",
    "                                plt.step(self.notes_played[0], self.notes_played[1], lw=0.8, where='post')\n",
    "\n",
    "                                y_values = [n for n in range(15)]\n",
    "                                notes_dict_proper = [\"C\", \"\", \"D\", \"\", \"E\", \"F\", \"\", \"G\", \"\", \"A\", \"\", \"B\", \"C\", \"\",\n",
    "                                                     \"D\"]\n",
    "                                plt.yticks(y_values, notes_dict_proper)\n",
    "\n",
    "                                plt.tight_layout()\n",
    "                                plt.show()\n",
    "                                try:\n",
    "                                    self.mic.close()\n",
    "                                    self.close()\n",
    "                                except:\n",
    "                                    pass\n",
    "\n",
    "                    elif self.indicate_inharmonic:\n",
    "                        print(\"inharmonic sound ({:.2f}) detected at {:.3f}s\"\n",
    "                              .format(portion_of_energy, time.time() - self.start_time))\n",
    "\n",
    "            display_only_note = True\n",
    "            if self.note_detected or not display_only_note:\n",
    "\n",
    "                self.line_top.set_data(self.time_vect, self.signal_to_show)  # plots the time signal, onset on middle\n",
    "                fft_frame = self.spectrum  # 1025 entries\n",
    "                fft_frame /= np.abs(fft_frame).max()\n",
    "                self.line_bottom.set_data(self.freq_vect, np.abs(fft_frame))\n",
    "\n",
    "                new_pitch = self.ffreq\n",
    "                precise_pitch = self.ffreq\n",
    "\n",
    "                self.ax_bottom.set_title(\"pitch = {:.2f} Hz ({})\".format(precise_pitch, self.note))\n",
    "                self.pitch_line.set_data((new_pitch, new_pitch),\n",
    "                                         self.ax_bottom.get_ylim())  # move the vertical pitch line\n",
    "\n",
    "                if self.iteration % 1 == 0:  # update plot only after every n chunks, if necessary\n",
    "                    self.main_figure.canvas.draw()  # refreshes the plots, takes the bulk of time\n",
    "                self.note_detected = False\n",
    "\n",
    "            self.signal_frame_pp3 = self.signal_frame_pp2[:]\n",
    "            self.signal_frame_pp2 = self.signal_frame_pp1[:]\n",
    "            self.signal_frame_pp1 = self.signal_frame_pp0[:]\n",
    "            self.energy_frame_pp1 = self.energy_frame_pp0[:]\n",
    "            self.rcoeff_frame_pp2 = self.rcoeff_frame_pp1[:]\n",
    "\n",
    "        self.iteration += 1\n",
    "        if self.iteration == 1:\n",
    "            print(\"start!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app = QtGui.QApplication(sys.argv)\n",
    "    window = LiveFFTWidget()\n",
    "    sys.exit(app.exec_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
